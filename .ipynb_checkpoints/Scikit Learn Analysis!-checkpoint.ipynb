{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import precision_score\n",
    "    \n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the data\n",
    "data = pd.read_csv('AggredgatedData.csv', sep=',', na_values=[\" \", \"\"], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = list(data.columns[1:-1])\n",
    "X = data[features]\n",
    "Y = data['2016ODabovenatavg']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##I. Model Selection Using Grid Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a useful tool to see which Classifier yields the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features=3, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Create space of candidate learning algorithms and their hyperparameters\n",
    "search_space = [{'classifier': [LogisticRegression()],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': [0.01, 1, 100]},\n",
    "                {'classifier': [RandomForestClassifier()],\n",
    "                 'classifier__n_estimators': [10, 50, 60, 100],\n",
    "                 'classifier__criterion': ['gini', 'entropy'],\n",
    "                 'classifier__max_features': [1, 2, 3],\n",
    "                 'classifier__oob_score': [True],\n",
    "                },\n",
    "                {'classifier': [SGDClassifier()],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__loss': ['hinge', 'log', 'modified_huber'],\n",
    "                 },\n",
    "                {'classifier': [SVC()],\n",
    "                 'classifier__kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "                 'classifier__C': [1, 10],\n",
    "                 'classifier__gamma': [0.01, 1, 10, 100]} ]\n",
    "\n",
    "clf0 = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "best_model = clf0.fit(X_train, Y_train)\n",
    "# View best model\n",
    "best_model.best_estimator_.get_params()['classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False,  True, False, False,\n",
       "       False,  True,  True,  True, False,  True, False, False,  True,\n",
       "       False, False,  True, False, False,  True, False, False, False,\n",
       "        True,  True, False, False, False,  True, False,  True, False,\n",
       "        True, False,  True,  True, False, False, False, False,  True,\n",
       "        True, False, False, False, False, False,  True, False], dtype=bool)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83018867924528306"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##II. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros: \n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "Cons:\n",
    "\n",
    "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cross Validation\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. \n",
    "\n",
    "This situation is called overfitting. \n",
    "\n",
    "To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. \n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. \n",
    "\n",
    "This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. \n",
    "\n",
    "To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "    * A model is trained using k-1 of the folds as training data;\n",
    "    * the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "    \n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as it is the case when fixing an arbitrary test set), which is a major advantage in problem such as inverse inference where the number of samples is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear kernel = Straight Line (hyperplane) as the decision boundary\n",
    "* rarely used in practice\n",
    "\n",
    "RBF = commonly used kernel in SVC\n",
    "2 parameters:\n",
    "* gamma\n",
    "* C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma:\n",
    "*  'spread' of the kernel and therefore the decision region.\n",
    "* low gamma -> the 'curve' of the decision boundary is very low and thus the decision region is very broad (underfitting)\n",
    "* gamma = 10 (The decision boundary starts to be highly effected by individual data points (i.e. variance)).\n",
    "* high gamma -> the 'curve' of the decision boundary is high, which creates islands of decision-boundaries around data points (overfitting)\n",
    "* If gamma is ‘auto’ then 1/n_features will be used instead.\n",
    "\n",
    "C:\n",
    "* penalty for misclassifying a data point\n",
    "* small C -> classifier is okay with misclassified data points (high bias, low variance)\n",
    "* big C -> classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance)\n",
    "\n",
    "C > 10 is too slow\n",
    "\n",
    "degree : int, optional (default=3)\n",
    "\n",
    "* Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear | Gamma: 0.01 | C: 1\n",
      "[ 0.70588235  0.6875      0.5         0.75        0.8125      0.5625      0.875\n",
      "  0.625       0.8         0.8       ]\n",
      "Accuracy: 0.71 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 0.01 | C: 10\n",
      "[ 0.52941176  0.8125      0.5625      0.625       0.75        0.6875      0.875\n",
      "  0.625       0.6         0.53333333]\n",
      "Accuracy: 0.66 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 1.00 | C: 1\n",
      "[ 0.70588235  0.6875      0.5         0.75        0.8125      0.5625      0.875\n",
      "  0.625       0.8         0.8       ]\n",
      "Accuracy: 0.71 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 1.00 | C: 10\n",
      "[ 0.52941176  0.8125      0.5625      0.625       0.75        0.6875      0.875\n",
      "  0.625       0.6         0.53333333]\n",
      "Accuracy: 0.66 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 10.00 | C: 1\n",
      "[ 0.70588235  0.6875      0.5         0.75        0.8125      0.5625      0.875\n",
      "  0.625       0.8         0.8       ]\n",
      "Accuracy: 0.71 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 10.00 | C: 10\n",
      "[ 0.52941176  0.8125      0.5625      0.625       0.75        0.6875      0.875\n",
      "  0.625       0.6         0.53333333]\n",
      "Accuracy: 0.66 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 100.00 | C: 1\n",
      "[ 0.70588235  0.6875      0.5         0.75        0.8125      0.5625      0.875\n",
      "  0.625       0.8         0.8       ]\n",
      "Accuracy: 0.71 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 100.00 | C: 10\n",
      "[ 0.52941176  0.8125      0.5625      0.625       0.75        0.6875      0.875\n",
      "  0.625       0.6         0.53333333]\n",
      "Accuracy: 0.66 (+/- 0.23)\n",
      "Kernel: rbf | Gamma: 0.01 | C: 1\n",
      "[ 0.76470588  0.6875      0.5625      0.6875      0.875       0.6875\n",
      "  0.6875      0.6875      0.6         0.8       ]\n",
      "Accuracy: 0.70 (+/- 0.17)\n",
      "Kernel: rbf | Gamma: 0.01 | C: 10\n",
      "[ 0.70588235  0.75        0.4375      0.625       0.875       0.6875\n",
      "  0.8125      0.75        0.66666667  0.66666667]\n",
      "Accuracy: 0.70 (+/- 0.22)\n",
      "Kernel: rbf | Gamma: 1.00 | C: 1\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 1.00 | C: 10\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 10.00 | C: 1\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 10.00 | C: 10\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 100.00 | C: 1\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 100.00 | C: 10\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: sigmoid | Gamma: 0.01 | C: 1\n",
      "[ 0.82352941  0.625       0.5625      0.6875      0.5625      0.5625\n",
      "  0.5625      0.6875      0.73333333  0.6       ]\n",
      "Accuracy: 0.64 (+/- 0.17)\n",
      "Kernel: sigmoid | Gamma: 0.01 | C: 10\n",
      "[ 0.76470588  0.5625      0.5625      0.625       0.5         0.5625\n",
      "  0.5625      0.625       0.73333333  0.53333333]\n",
      "Accuracy: 0.60 (+/- 0.16)\n",
      "Kernel: sigmoid | Gamma: 1.00 | C: 1\n",
      "[ 0.76470588  0.5625      0.5         0.625       0.8125      0.625       0.8125\n",
      "  0.6875      0.73333333  0.66666667]\n",
      "Accuracy: 0.68 (+/- 0.20)\n",
      "Kernel: sigmoid | Gamma: 1.00 | C: 10\n",
      "[ 0.82352941  0.5         0.4375      0.625       0.75        0.625       0.75\n",
      "  0.6875      0.8         0.66666667]\n",
      "Accuracy: 0.67 (+/- 0.24)\n",
      "Kernel: sigmoid | Gamma: 10.00 | C: 1\n",
      "[ 0.82352941  0.5         0.5625      0.625       0.8125      0.5625      0.75\n",
      "  0.6875      0.8         0.66666667]\n",
      "Accuracy: 0.68 (+/- 0.22)\n",
      "Kernel: sigmoid | Gamma: 10.00 | C: 10\n",
      "[ 0.82352941  0.5         0.5         0.625       0.8125      0.5625      0.75\n",
      "  0.6875      0.8         0.6       ]\n",
      "Accuracy: 0.67 (+/- 0.24)\n",
      "Kernel: sigmoid | Gamma: 100.00 | C: 1\n",
      "[ 0.82352941  0.5         0.5         0.625       0.8125      0.5625\n",
      "  0.6875      0.6875      0.8         0.66666667]\n",
      "Accuracy: 0.67 (+/- 0.23)\n",
      "Kernel: sigmoid | Gamma: 100.00 | C: 10\n",
      "[ 0.82352941  0.5         0.5         0.625       0.8125      0.5625\n",
      "  0.6875      0.6875      0.8         0.6       ]\n",
      "Accuracy: 0.66 (+/- 0.23)\n"
     ]
    }
   ],
   "source": [
    "kernels = ['linear', 'rbf', 'sigmoid']\n",
    "\n",
    "for i in kernels:\n",
    "    for j, C in enumerate((0.01, 1, 10, 100)):\n",
    "        for k, D in enumerate((1, 10)):\n",
    "            clf = SVC(C=D, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                      decision_function_shape='ovr', degree=3, gamma=C, kernel=i,\n",
    "                      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "                      tol=0.001, verbose=False)\n",
    "            clf.fit(X_train, Y_train) \n",
    "            scores = cross_val_score(clf, X, Y, cv = 10)\n",
    "\n",
    "            print (\"Kernel: %s | Gamma: %0.2f | C: %i\" % (i, C, D))\n",
    "            print scores\n",
    "            print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal Model\n",
    "\n",
    "Kernel: rbf | Gamma: 0.01 | C: 1\n",
    "\n",
    "[ 0.76470588  0.6875      0.5625      0.6875      0.875       0.6875\n",
    "  0.6875      0.6875      0.6         0.8       ]\n",
    "  \n",
    "Accuracy: 0.70 (+/- 0.17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf0 = SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
    "          decision_function_shape='ovr', degree=3, gamma=0.01, kernel='linear',\n",
    "          max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "          tol=0.001, verbose=False)\n",
    "clf0.fit(X_train, Y_train) \n",
    "scores = cross_val_score(clf0, X, Y, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71 (+/- 0.23)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.63848801e-01,  -6.68540451e-02,   3.09387286e-01,\n",
       "         -2.05750837e-01,   1.34634014e+00,   1.40482994e-01,\n",
       "          7.49506633e-01,   1.59324072e+00,   3.68813954e-01,\n",
       "          3.97217579e-02,  -3.60074262e-01,   1.83569736e-01,\n",
       "         -2.72166587e-01,  -2.27081181e-01,   8.76271007e-03,\n",
       "          3.60658592e-01,  -4.52500113e-01,  -2.06844360e-01,\n",
       "          2.52252443e-02,   3.51946070e-01,  -8.41550902e-01,\n",
       "          5.08848354e-01,  -2.47331720e-01,   7.35406646e-01,\n",
       "         -1.60632132e-02,  -1.61889112e-01,   5.63491887e-01,\n",
       "         -3.35239154e-01,   8.46799623e-04,   2.23128289e-01,\n",
       "          5.84754921e-02,  -2.64266052e-01,   1.39304776e-01,\n",
       "          1.76294694e-02,  -5.67709251e-02,  -7.26662105e-01,\n",
       "          1.72026303e-03,  -1.46720138e-02,   5.96602027e-01,\n",
       "          2.40489622e-03]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf0.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.91970688])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf0.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##II. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary classifier\n",
    "\n",
    "L1 regularization (also called least absolute deviations) \n",
    "* push feature coefficients to 0, creating a method for feature selection. \n",
    "* as C decreases, more coefficients become 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False  True False  True  True False False  True  True False\n",
      "  True False  True  True  True  True False False  True  True False False\n",
      " False False  True  True  True False  True False  True False  True False\n",
      "  True False False False False False False False  True False  True False\n",
      "  True False  True  True False]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(penalty='l1', dual=False, tol=0.01, C=100.0, fit_intercept=True, \n",
    "                   intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', \n",
    "                   max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "clf1.fit(X_train, Y_train) \n",
    "\n",
    "print(clf1.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.64705882  0.6875      0.5         0.5         0.6875      0.6875      0.875\n",
      "  0.5625      0.6         0.73333333]\n",
      "Accuracy: 0.65 (+/- 0.22)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf1, X, Y, cv = 10)\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=100.00\n",
      "Sparsity with L1 penalty: 5.00%\n",
      "score with L1 penalty: 0.8491\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.8491\n",
      "C=1.00\n",
      "Sparsity with L1 penalty: 57.50%\n",
      "score with L1 penalty: 0.7736\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.7925\n",
      "C=0.01\n",
      "Sparsity with L1 penalty: 95.00%\n",
      "score with L1 penalty: 0.6415\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.7421\n"
     ]
    }
   ],
   "source": [
    "#Comparison of the sparsity (% of zero coefficients) of solutions when L1 and L2 penalty \n",
    "#are used for different values of C. \n",
    "#We can see that large values of C give more freedom to the model. \n",
    "#Conversely, smaller values of C constrain the model more. \n",
    "#In the L1 penalty case, this leads to sparser solutions.\n",
    "\n",
    "for i, C in enumerate((100, 1, 0.01)):\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n",
    "    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n",
    "    clf_l1_LR.fit(X, Y)\n",
    "    clf_l2_LR.fit(X, Y)\n",
    "\n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
    "\n",
    "    print(\"C=%.2f\" % C)\n",
    "    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)\n",
    "    print(\"score with L1 penalty: %.4f\" % clf_l1_LR.score(X, Y))\n",
    "    print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR)\n",
    "    print(\"score with L2 penalty: %.4f\" % clf_l2_LR.score(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "* Efficiency.\n",
    "* Ease of implementation (lots of opportunities for code tuning).\n",
    "\n",
    "Cons:\n",
    "* requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "* sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
    "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
    "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
    "       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
    "\n",
    "clf3.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False, False, False, False, False, False,\n",
       "        True,  True, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False,  True, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -5.23217789e+01,  -3.27011118e+01,  -2.61608895e+01,\n",
       "         -2.61608895e+01,   8.50228908e+01,   5.88620013e+01,\n",
       "          1.95828386e+02,   2.36071445e+02,   1.31615923e+02,\n",
       "          1.14939664e+01,   1.12390395e+01,  -2.20715014e+00,\n",
       "         -4.77775141e+01,  -4.15829428e+01,   3.31181442e+01,\n",
       "          2.51492789e+01,  -2.71001948e+01,  -3.98133628e+01,\n",
       "          5.95502310e+00,   2.28872008e+01,   4.94410705e+00,\n",
       "          9.04832470e+00,   7.46644264e+00,   3.59557058e+01,\n",
       "         -1.45118112e+01,  -5.14007471e+00,   3.04663837e+00,\n",
       "         -1.71934843e+01,   4.97520380e+01,   4.02451916e+01,\n",
       "          3.69608367e+01,  -1.71696321e+02,  -6.73574243e+01,\n",
       "          4.06115108e-01,  -6.76913015e-01,  -3.44146501e+01,\n",
       "         -1.75283087e-01,  -3.19077067e-01,   3.70045782e+01,\n",
       "          6.77973026e-02]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-42.41118979])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.intercept_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.76470588  0.875       0.5         0.625       0.5625      0.5625      0.75\n",
      "  0.75        0.66666667  0.66666667]\n",
      "Accuracy: 0.67 (+/- 0.22)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf3, X, Y, cv = 10)\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##IV. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. \n",
    "\n",
    "The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).\n",
    "\n",
    "Pros:\n",
    "\n",
    "* It runs efficiently on large data bases.\n",
    "* It can handle thousands of input variables without variable deletion.\n",
    "* It gives estimates of what variables are important in the classification.\n",
    "* It generates an internal unbiased estimate of the generalization error as the forest building progresses.\n",
    "* It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n",
    "* It has methods for balancing error in class population unbalanced data sets.\n",
    "* Generated forests can be saved for future use on other data.\n",
    "* Prototypes are computed that give information about the relation between the variables and the classification.\n",
    "* It computes proximities between pairs of cases that can be used in clustering, locating outliers, or (by scaling) give interesting views of the data.\n",
    "* The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection.\n",
    "* It offers an experimental method for detecting variable interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Trees in RF\n",
    "\n",
    "\n",
    "Random forests does not overfit. You can run as many trees as you want. It is fast. Running on a data set with 50,000 cases and 100 variables, it produced 100 trees in 11 minutes on a 800Mhz machine. For large data sets the major memory requirement is the storage of the data itself, and three integer arrays with the same dimensions as the data. If proximities are calculated, storage requirements grow as the number of cases times the number of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:\n",
    "\n",
    "Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree.\n",
    "\n",
    "* https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#remarks\n",
    "\n",
    "The RandomForestClassifier is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training observations z_i = (x_i, y_i). \n",
    "\n",
    "The out-of-bag (OOB) error is the average error for each z_i calculated using predictions from the trees that do not contain z_i in their respective bootstrap sample. \n",
    "\n",
    "This allows the RandomForestClassifier to be fit and validated whilst being trained. \n",
    "\n",
    "* T. Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning Ed. 2”, p592-593, Springer, 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4 = RandomForestClassifier(n_estimators=50, criterion='entropy', max_depth=None, \n",
    "                              min_samples_split=2, \n",
    "                       min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                              max_features='auto', \n",
    "                       max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                       bootstrap=True, oob_score=True, n_jobs=1, random_state=None, verbose=0, \n",
    "                       warm_start=False, class_weight=None)\n",
    "            \n",
    "clf4.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92452830188679247"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4.score(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
