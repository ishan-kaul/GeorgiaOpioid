{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import precision_score\n",
    "    \n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the data\n",
    "data = pd.read_csv('AggredgatedData.csv', sep=',', na_values=[\" \", \"\"], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = list(data.columns[1:-1])\n",
    "X = data[features]\n",
    "Y = data['2016ODabovenatavg']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##I. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros: \n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "Cons:\n",
    "\n",
    "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear kernel = Straight Line (hyperplane) as the decision boundary\n",
    "* rarely used in practice\n",
    "\n",
    "RBF = commonly used kernel in SVC\n",
    "2 parameters:\n",
    "* gamma\n",
    "* C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma:\n",
    "*  'spread' of the kernel and therefore the decision region.\n",
    "* low gamma -> the 'curve' of the decision boundary is very low and thus the decision region is very broad (underfitting)\n",
    "* gamma = 10 (The decision boundary starts to be highly effected by individual data points (i.e. variance)).\n",
    "* high gamma -> the 'curve' of the decision boundary is high, which creates islands of decision-boundaries around data points (overfitting)\n",
    "\n",
    "C:\n",
    "* penalty for misclassifying a data point\n",
    "* small C -> classifier is okay with misclassified data points (high bias, low variance)\n",
    "* big C -> classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance)\n",
    "\n",
    "C > 10 is too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear | Gamma: 0.01 | C: 1\n",
      "[ 0.70588235  0.6875      0.5         0.75        0.8125      0.5625      0.875\n",
      "  0.625       0.8         0.8       ]\n",
      "Accuracy: 0.71 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 0.01 | C: 10\n",
      "[ 0.52941176  0.8125      0.5625      0.625       0.75        0.6875      0.875\n",
      "  0.625       0.6         0.53333333]\n",
      "Accuracy: 0.66 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 1.00 | C: 1\n",
      "[ 0.70588235  0.6875      0.5         0.75        0.8125      0.5625      0.875\n",
      "  0.625       0.8         0.8       ]\n",
      "Accuracy: 0.71 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 1.00 | C: 10\n",
      "[ 0.52941176  0.8125      0.5625      0.625       0.75        0.6875      0.875\n",
      "  0.625       0.6         0.53333333]\n",
      "Accuracy: 0.66 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 10.00 | C: 1\n",
      "[ 0.70588235  0.6875      0.5         0.75        0.8125      0.5625      0.875\n",
      "  0.625       0.8         0.8       ]\n",
      "Accuracy: 0.71 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 10.00 | C: 10\n",
      "[ 0.52941176  0.8125      0.5625      0.625       0.75        0.6875      0.875\n",
      "  0.625       0.6         0.53333333]\n",
      "Accuracy: 0.66 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 100.00 | C: 1\n",
      "[ 0.70588235  0.6875      0.5         0.75        0.8125      0.5625      0.875\n",
      "  0.625       0.8         0.8       ]\n",
      "Accuracy: 0.71 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 100.00 | C: 10\n",
      "[ 0.52941176  0.8125      0.5625      0.625       0.75        0.6875      0.875\n",
      "  0.625       0.6         0.53333333]\n",
      "Accuracy: 0.66 (+/- 0.23)\n",
      "Kernel: rbf | Gamma: 0.01 | C: 1\n",
      "[ 0.76470588  0.6875      0.5625      0.6875      0.875       0.6875\n",
      "  0.6875      0.6875      0.6         0.8       ]\n",
      "Accuracy: 0.70 (+/- 0.17)\n",
      "Kernel: rbf | Gamma: 0.01 | C: 10\n",
      "[ 0.70588235  0.75        0.4375      0.625       0.875       0.6875\n",
      "  0.8125      0.75        0.66666667  0.66666667]\n",
      "Accuracy: 0.70 (+/- 0.22)\n",
      "Kernel: rbf | Gamma: 1.00 | C: 1\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 1.00 | C: 10\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 10.00 | C: 1\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 10.00 | C: 10\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 100.00 | C: 1\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 100.00 | C: 10\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: sigmoid | Gamma: 0.01 | C: 1\n",
      "[ 0.82352941  0.625       0.5625      0.6875      0.5625      0.5625\n",
      "  0.5625      0.6875      0.73333333  0.6       ]\n",
      "Accuracy: 0.64 (+/- 0.17)\n",
      "Kernel: sigmoid | Gamma: 0.01 | C: 10\n",
      "[ 0.76470588  0.5625      0.5625      0.625       0.5         0.5625\n",
      "  0.5625      0.625       0.73333333  0.53333333]\n",
      "Accuracy: 0.60 (+/- 0.16)\n",
      "Kernel: sigmoid | Gamma: 1.00 | C: 1\n",
      "[ 0.76470588  0.5625      0.5         0.625       0.8125      0.625       0.8125\n",
      "  0.6875      0.73333333  0.66666667]\n",
      "Accuracy: 0.68 (+/- 0.20)\n",
      "Kernel: sigmoid | Gamma: 1.00 | C: 10\n",
      "[ 0.82352941  0.5         0.4375      0.625       0.75        0.625       0.75\n",
      "  0.6875      0.8         0.66666667]\n",
      "Accuracy: 0.67 (+/- 0.24)\n",
      "Kernel: sigmoid | Gamma: 10.00 | C: 1\n",
      "[ 0.82352941  0.5         0.5625      0.625       0.8125      0.5625      0.75\n",
      "  0.6875      0.8         0.66666667]\n",
      "Accuracy: 0.68 (+/- 0.22)\n",
      "Kernel: sigmoid | Gamma: 10.00 | C: 10\n",
      "[ 0.82352941  0.5         0.5         0.625       0.8125      0.5625      0.75\n",
      "  0.6875      0.8         0.6       ]\n",
      "Accuracy: 0.67 (+/- 0.24)\n",
      "Kernel: sigmoid | Gamma: 100.00 | C: 1\n",
      "[ 0.82352941  0.5         0.5         0.625       0.8125      0.5625\n",
      "  0.6875      0.6875      0.8         0.66666667]\n",
      "Accuracy: 0.67 (+/- 0.23)\n",
      "Kernel: sigmoid | Gamma: 100.00 | C: 10\n",
      "[ 0.82352941  0.5         0.5         0.625       0.8125      0.5625\n",
      "  0.6875      0.6875      0.8         0.6       ]\n",
      "Accuracy: 0.66 (+/- 0.23)\n"
     ]
    }
   ],
   "source": [
    "kernels = ['linear', 'rbf', 'sigmoid']\n",
    "#gamma\n",
    "#If gamma is ‘auto’ then 1/n_features will be used instead.\n",
    "\n",
    "#degree : int, optional (default=3)\n",
    "#Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
    "\n",
    "for i in kernels:\n",
    "    for j, C in enumerate((0.01, 1, 10, 100)):\n",
    "        for k, D in enumerate((1, 10)):\n",
    "            clf = SVC(C=D, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                      decision_function_shape='ovr', degree=3, gamma=C, kernel=i,\n",
    "                      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "                      tol=0.001, verbose=False)\n",
    "            clf.fit(X_train, Y_train) \n",
    "            scores = cross_val_score(clf, X, Y, cv = 10)\n",
    "\n",
    "            print (\"Kernel: %s | Gamma: %0.2f | C: %i\" % (i, C, D))\n",
    "            print scores\n",
    "            print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##II. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binary classifier\n",
    "\n",
    "L1 regularization (also called least absolute deviations) \n",
    "* push feature coefficients to 0, creating a method for feature selection. \n",
    "* as C decreases, more coefficients become 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False  True False  True  True False False  True  True False\n",
      "  True False  True  True  True  True False False  True  True False False\n",
      " False False  True  True  True False  True False  True False  True False\n",
      "  True False False False False False False  True  True False  True False\n",
      "  True False  True  True False]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(penalty='l1', dual=False, tol=0.01, C=100.0, fit_intercept=True, \n",
    "                   intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', \n",
    "                   max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "clf1.fit(X_train, Y_train) \n",
    "\n",
    "print(clf1.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.64705882  0.6875      0.5         0.625       0.6875      0.6875      0.875\n",
      "  0.5625      0.6         0.8       ]\n",
      "Accuracy: 0.67 (+/- 0.21)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf1, X, Y, cv = 10)\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=100.00\n",
      "Sparsity with L1 penalty: 5.00%\n",
      "score with L1 penalty: 0.8491\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.8302\n",
      "C=1.00\n",
      "Sparsity with L1 penalty: 57.50%\n",
      "score with L1 penalty: 0.7799\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.7925\n",
      "C=0.01\n",
      "Sparsity with L1 penalty: 95.00%\n",
      "score with L1 penalty: 0.6415\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.7421\n"
     ]
    }
   ],
   "source": [
    "#Comparison of the sparsity (% of zero coefficients) of solutions when L1 and L2 penalty \n",
    "#are used for different values of C. \n",
    "#We can see that large values of C give more freedom to the model. \n",
    "#Conversely, smaller values of C constrain the model more. \n",
    "#In the L1 penalty case, this leads to sparser solutions.\n",
    "\n",
    "for i, C in enumerate((100, 1, 0.01)):\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n",
    "    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n",
    "    clf_l1_LR.fit(X, Y)\n",
    "    clf_l2_LR.fit(X, Y)\n",
    "\n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
    "\n",
    "    print(\"C=%.2f\" % C)\n",
    "    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)\n",
    "    print(\"score with L1 penalty: %.4f\" % clf_l1_LR.score(X, Y))\n",
    "    print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR)\n",
    "    print(\"score with L2 penalty: %.4f\" % clf_l2_LR.score(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "* Efficiency.\n",
    "* Ease of implementation (lots of opportunities for code tuning).\n",
    "\n",
    "Cons:\n",
    "* requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "* sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
    "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
    "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
    "       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
    "\n",
    "clf3.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False,  True, False,  True,  True, False, False,\n",
       "        True,  True, False,  True, False,  True,  True,  True,  True,\n",
       "       False, False,  True,  True, False,  True, False,  True,  True,\n",
       "        True, False,  True,  True, False,  True, False,  True, False,\n",
       "        True,  True,  True,  True, False, False,  True,  True,  True,\n",
       "        True,  True,  True, False, False,  True,  True, False], dtype=bool)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.57815566e+01,   1.50879963e-14,   2.61608895e+01,\n",
       "         -2.61608895e+01,   1.04643558e+02,   3.27011118e+01,\n",
       "          2.00508997e+02,   2.95783550e+02,   2.17630119e+02,\n",
       "          1.01921571e+01,   1.11889466e+01,   4.13377811e+00,\n",
       "         -6.19553237e+01,   2.81719876e+01,  -1.18801893e+01,\n",
       "          5.79328344e+01,  -3.62768771e+01,  -3.85361927e+01,\n",
       "          7.65269047e+00,   1.33746356e+01,  -5.86051608e+00,\n",
       "          1.57607692e+01,   4.35397517e-01,   3.85571200e+01,\n",
       "         -1.08133255e+01,  -1.17352363e+01,   1.26833722e+01,\n",
       "         -2.73213734e+01,   4.60294511e+01,   2.76116816e+01,\n",
       "          4.02134762e+01,  -6.55977122e+01,   4.21165986e+01,\n",
       "          4.35873120e-01,  -9.89535644e-01,  -2.82733813e+01,\n",
       "          1.66400233e-01,  -5.24451644e-01,   4.33485939e+01,\n",
       "          6.96464951e-02]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-37.4209551])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.intercept_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.64705882  0.6875      0.5         0.5625      0.625       0.625       0.4375\n",
      "  0.6875      0.6         0.66666667]\n",
      "Accuracy: 0.60 (+/- 0.16)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf3, X, Y, cv = 10)\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##IV. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf4 = RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=None, \n",
    "                              min_samples_split=2, \n",
    "                       min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                              max_features='auto', \n",
    "                       max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                       bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, \n",
    "                       warm_start=False, class_weight=None)\n",
    "\n",
    "clf4.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##V. Model Selection Using Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=3, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Create space of candidate learning algorithms and their hyperparameters\n",
    "search_space = [{'classifier': [LogisticRegression()],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': [0.01, 1, 100]},\n",
    "                {'classifier': [RandomForestClassifier()],\n",
    "                 'classifier__n_estimators': [10, 100, 1000],\n",
    "                 'classifier__max_features': [1, 2, 3]},\n",
    "                {'classifier': [SGDClassifier()],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__loss': ['hinge', 'log', 'modified_huber']\n",
    "                 },\n",
    "                {'classifier': [SVC()],\n",
    "                 'classifier__kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "                 'classifier__C': [1, 10],\n",
    "                 'classifier__gamma': [0.01, 1, 10, 100]} ]\n",
    "\n",
    "#SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "#        eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
    "#        learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
    "#        n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
    "#        shuffle=True, tol=None, verbose=0, warm_start=False)\n",
    "\n",
    "\n",
    "clf5 = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "best_model = clf5.fit(X_train, Y_train)\n",
    "# View best model\n",
    "best_model.best_estimator_.get_params()['classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False, False, False,  True, False, False,\n",
       "        True,  True,  True,  True, False,  True, False, False,  True,\n",
       "       False, False,  True, False, False,  True, False, False,  True,\n",
       "        True,  True, False, False, False,  True, False,  True, False,\n",
       "        True, False,  True,  True, False, False, False, False,  True,\n",
       "        True, False, False, False, False, False,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90566037735849059"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
